
siamo partiti dal concetto di 'neurone' e siamo a arrivati a quello di percettrone, poi abbiamo introdotto i multilayer perceptron

tre architetture principali:
- multilayer perceptron -> limit: i should consider how many feature in input as context
- deep neural network
- recurrent neural network

main advantages of recurrent: at time $t$, the result is dependenent of all $t-x$ responses (all the past words + following words to make a decision at time $t$)

a bidirectional nnn lookup at the past and forward in the future, considering the entire context

talking about rnn use cases, such as image captioning, **text classification**. More in general, all "sequence-to-sequence" problems kind, i didn't understand what that means lol. The "sequence" refer to the number of things feeded in the rnn or outed (sequence in input and in output). The two sequence may differ: think about a translation: more words can come out from the original language.

a possible way is the encoder - decoder architecture: the encoder builds a representation using the inputs and then the decoder tries to build another fitting representations (for example, in another language). However this architecture is kinda slow to train.

rnn can't be parallelized! because to make a decision at time $t$ i need to wait all $t-x$ decisions! this is why encoder-decoder is quite inefficient.

